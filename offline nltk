def offline_nltk_pipeline(text, nltk_path=r"C:\path\to\nltk_data"):
    import nltk, os, pickle
    nltk.data.path.append(nltk_path)

    # Sentence tokenizer
    punkt_path = os.path.join(nltk_path, "tokenizers", "punkt", "english.pickle")
    with open(punkt_path, "rb") as f:
        sent_tokenizer = pickle.load(f)

    from nltk.tokenize import TreebankWordTokenizer
    from nltk import pos_tag, ne_chunk
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    from nltk.sentiment import SentimentIntensityAnalyzer

    word_tokenizer = TreebankWordTokenizer()
    stemmer = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words("english"))
    sia = SentimentIntensityAnalyzer()

    results = []
    for sent in sent_tokenizer.tokenize(text):
        tokens = word_tokenizer.tokenize(sent)
        pos = pos_tag(tokens)
        ner = ne_chunk(pos)
        stems = [stemmer.stem(t) for t in tokens]
        lemmas = [lemmatizer.lemmatize(t) for t in tokens]
        filtered = [t for t in tokens if t.lower() not in stop_words]
        sentiment = sia.polarity_scores(sent)

        results.append({
            "sentence": sent,
            "tokens": tokens,
            "pos": pos,
            "ner": ner,
            "stems": stems,
            "lemmas": lemmas,
            "filtered": filtered,
            "sentiment": sentiment
        })
    return results
