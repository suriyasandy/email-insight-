import pandas as pd
import os

csv_path = "output.csv"

if os.path.exists(csv_path):
    # Read existing data
    existing = pd.read_csv(csv_path)

    # Get union of all columns (existing + new)
    all_cols = list(set(existing.columns).union(set(df.columns)))

    # Align both dataframes to the same column set
    existing = existing.reindex(columns=all_cols)
    df = df.reindex(columns=all_cols)

    # Append, drop duplicates (optional)
    combined = pd.concat([existing, df], ignore_index=True).drop_duplicates().reset_index(drop=True)

    # Save back with updated headers
    combined.to_csv(csv_path, index=False)
else:
    # Create new file with headers
    df.to_csv(csv_path, index=False)



def extract_email_chain_from_outlook(self):
    trade_ids_str = None
    df = pd.DataFrame(columns=self.selected_columns)
    if hasattr(self, "trade_ids_var") or isinstance(self.trade_ids_var, tk.StringVar):
        trade_ids_str = self.trade_ids_var.get().strip()
    if not trade_ids_str:
        file_path = self.file_path_var.get().strip()
        if not file_path or not os.path.exists(file_path):
            messagebox.showwarning("Input Error", "Please upload a valid file/input trade_id")
            return
        self.trade_ids_var.set(self.process_trade_file(file_path))
        trade_ids_str = self.trade_ids_var.get().strip()

    trade_ids = [tid.strip() for tid in re.split(",|\\s", trade_ids_str) if tid.strip()]

    self.progress.start()
    self.clear_results()
    self.emails = []
    self.unfound_trades = []

    def worker():
        pythoncom.CoInitialize()
        try:
            folder = self.connect_to_outlook()
            if not folder:
                self.root.after(0, lambda: self.finish_extraction([], "Could not connect to folder"))
                return

            today = datetime.now()
            days_back = 3
            date_fmt, use_ampm = self.get_region_datefmt_ampm()
            logger.info(f"Detected locale datefmt: {date_fmt}, Use AM/PM: {use_ampm}")
            all_items = []

            for days_ago in range(1, days_back + 1):
                d = today - timedelta(days=days_ago)
                start_time, end_time = self.restrict_datetime_strings(d, date_fmt, use_ampm)
                filter_str = f"[ReceivedTime] >= '{start_time}' AND [ReceivedTime] <= '{end_time}'"
                logger.info(f"Trying filter: {filter_str}")
                items = folder.Items.Restrict(filter_str)
                items_list = self.process_items_in_batches(items, batch_size=150)
                if not items_list and use_ampm:
                    am_l = d.replace(hour=1).strftime('%p').lower()
                    pm_l = d.replace(hour=13).strftime('%p').lower()
                    start_time_l = f"{d.strftime(date_fmt)} 12:00:00 {am_l}"
                    end_time_l = f"{d.strftime(date_fmt)} 11:59:59 {pm_l}"
                    filter_str_l = f"[ReceivedTime] >= '{start_time_l}' AND [ReceivedTime] <= '{end_time_l}'"
                    logger.info(f"Trying filter (lowercase): {filter_str_l}")
                    items = folder.Items.Restrict(filter_str_l)
                    items_list = self.process_items_in_batches(items, batch_size=150)
                if items_list:
                    logger.info(f"Found {len(items_list)} emails for {d.strftime(date_fmt)}")
                    all_items.extend([entry["item"] for entry in items_list])
                else:
                    logger.info(f"No emails for {d.strftime(date_fmt)}")

            # --------- Single-pass: group emails per trade ID ---------
            tid_re = re.compile(r"|".join(re.escape(tid) for tid in trade_ids), re.IGNORECASE)
            matches_by_tradeid = {tid: [] for tid in trade_ids}
            all_emails = []

            for item in all_items:
                try:
                    if getattr(item, "Class", None) == 43:
                        subject = item.Subject or ""
                        body = item.Body or ""
                        html_body = ""
                        try:
                            html_body = item.HTMLBody
                        except Exception:
                            pass
                        search_zone = f"{subject}\n{body}\n{html_body}"
                        found_tids = set(m.lower() for m in tid_re.findall(search_zone))
                        for found_tid in found_tids:
                            for orig_tid in trade_ids:
                                if orig_tid.lower() == found_tid:
                                    email_obj = {
                                        'trade_id': orig_tid,
                                        'subject': subject,
                                        'sender': getattr(item, "SenderName", ""),
                                        'recipient': getattr(item, "To", ""),
                                        'date': item.ReceivedTime.strftime("%Y-%m-%d %H:%M:%S") if item.ReceivedTime else "",
                                        'body': body,
                                        'html_body': html_body,
                                    }
                                    matches_by_tradeid[orig_tid].append(email_obj)
                                    all_emails.append(email_obj)
                except Exception:
                    pass

            # Rebuild self.emails so downstream logic (entity extraction, selection) works UNCHANGED
            self.emails = [{"trade_id": tid, "emails": matches_by_tradeid[tid]} for tid in trade_ids]

            found_trade_ids = set(mail['trade_id'] for mail in all_emails)
            self.unfound_trades = [tid for tid in trade_ids if tid not in found_trade_ids]
            logger.info(f"{self.unfound_trades} - Not Found In Email")
            for tid in self.unfound_trades:
                filename = f"{tid}_email_entities_export_{datetime.now().strftime('%m_%d_%Y')}.csv"
                out_file = os.path.join(self.temp_path, filename)
                df.to_csv(out_file, index=False)

            gc.collect()
            self.root.after(0, lambda: self.finish_extraction(all_emails, None))
        except Exception as e:
            gc.collect()
            self.root.after(0, lambda: self.finish_extraction([], str(e)))
        finally:
            pythoncom.CoUninitialize()

    threading.Thread(target=worker, daemon=True).start()



#activity type
def extract_entities(self):
    """Extract entities with activity type tracking"""
    if self.selected_email is None:
        messagebox.showwarning("No Selection", "Please select an email from the chain first")
        return
    
    try:
        entity_json = self.entity_text.get(1.0, tk.END).strip()
        self.entity_definitions = json.loads(entity_json)
    except Exception as e:
        messagebox.showerror("Error", f"Invalid JSON format: {str(e)}")
        return

    email_data = self.selected_email
    entities = {}

    # Define activity type keywords (add more as needed)
    activity_keywords = [
        "NEW TRADE", "RESTRUCTURE", "UNWIND", "TERMINATION", 
        "AMENDMENT", "NOVATION", "ASSIGNMENT", "EXERCISE"
    ]

    email_lines = email_data['body'].splitlines()
    email_lines = list(filter(None, email_lines))  # Remove completely empty strings
    
    current_activity = None  # Track current activity type
    
    for line in email_lines:
        line = line.strip()
        
        # Skip empty lines but retain activity context
        if not line:
            continue
            
        # Check if line contains an activity type keyword
        line_upper = line.upper()
        for keyword in activity_keywords:
            if keyword in line_upper:
                current_activity = keyword
                logger.info(f"Activity type changed to: {current_activity}")
                break
        
        # Process entity definitions
        for entity_def in self.entity_definitions.get("entities", []):
            entity_name = entity_def.get("name", "")
            entity_type = entity_def.get("type", "")
            
            if entity_type == "pattern":
                patterns = entity_def.get("patterns", [])
                if entity_name == "PackageDetails":
                    # Special handling to extract named groups from full line pattern
                    for pattern in patterns:
                        try:
                            if self.attribute_count_check(pattern, line):
                                regex = re.compile(pattern, re.IGNORECASE)
                                matches = regex.finditer(line)
                                for match in matches:
                                    groupdict = match.groupdict()
                                    # Add activity type to extracted data
                                    if current_activity:
                                        groupdict['ActivityType'] = current_activity
                                    for k, v in groupdict.items():
                                        if v:
                                            entities.setdefault(k, []).append(f"{v} (Body)")
                        except Exception as e:
                            logger.info(f"Error in PackageDetails pattern extraction: {e}")
                else:
                    # Standard pattern extraction
                    subject_vals = self.extract_with_patterns(email_data['subject'], patterns)
                    body_vals = self.extract_with_patterns(line, patterns)  # Process line by line
                    all_vals = []
                    for v in subject_vals:
                        all_vals.append(f"{v} (Subject)")
                    for v in body_vals:
                        all_vals.append(f"{v} (Body)")
                    if all_vals:
                        entities[entity_name] = all_vals

            elif entity_type == "gazetteer":
                gazetteer_vals = entity_def.get("values", [])
                subject_vals = self.extract_with_gazetteer(email_data['subject'], gazetteer_vals)
                body_vals = self.extract_with_gazetteer(line, gazetteer_vals)
                all_vals = []
                for v in subject_vals:
                    all_vals.append(f"{v} (Subject)")
                for v in body_vals:
                    all_vals.append(f"{v} (Body)")
                if all_vals:
                    entities[entity_name] = all_vals

    self.entity_result_text.delete(1.0, tk.END)
    if entities:
        for entity_name, vals in entities.items():
            self.entity_result_text.insert(tk.END, f"{entity_name}:\n")
            for v in vals:
                self.entity_result_text.insert(tk.END, f"  - {v}\n")
            self.entity_result_text.insert(tk.END, "\n")
    else:
        self.entity_result_text.insert(tk.END, "No entities found in this email")


def attribute_count_check(self, pattern, line):
    """Validate that pattern's group count matches the line's token count"""
    group_count = len(re.compile(pattern).groupindex)
    text_count = len(line.split())
    if text_count == group_count:
        return 1
    else:
        return 0

